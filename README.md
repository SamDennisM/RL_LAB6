# ğŸ¤– Q-Learning & Double Q-Learning â€” Advanced Reinforcement Learning in Gridworld  
> **Exploration vs. Exploitation â€¢ Dynamic Obstacles â€¢ Multi-Agent Ready Framework**

<p align="center">
  <img src="https://img.shields.io/badge/Algorithm-Q%20Learning-1E90FF?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Variant-Double%20Q%20Learning-FF69B4?logo=pytorch&logoColor=white" />
  <img src="https://img.shields.io/badge/Category-Reinforcement%20Learning-8A2BE2" />
  <img src="https://img.shields.io/badge/Environment-Custom%20Gridworld-green" />
  <img src="https://img.shields.io/badge/Visualization-Matplotlib-yellow" />
</p>

---

## âœ¨ Overview

This project implements **Q-Learning** and its robust variant **Double Q-Learning** in a **stochastic Gridworld environment** filled with **holes**, **obstacles**, and **goals**.  
It visualizes **policy evolution**, **learning curves**, and **reward optimization** â€” highlighting how agents learn optimal behavior through trial and error.

ğŸ’¡ **Goal:** Enable an agent to navigate complex terrain while maximizing total rewards using reinforcement learning principles.

---

## ğŸš€ Features

- âš™ï¸ **Q-Learning & Double Q-Learning**
  - Compare traditional vs. double estimators for value correction  
- ğŸ§­ **Customizable Gridworld**
  - Multiple goals, holes, traps, and dynamic start positions  
- ğŸ§© **Adaptive Exploration**
  - Îµ-greedy policy with exponential decay for stable convergence  
- ğŸ§® **Reward Optimization**
  - Configurable reward matrices for different training difficulties  
- ğŸ“Š **Visualization Suite**
  - Training curves, reward histograms, and policy maps  
- ğŸ’¾ **Exportable Knowledge**
  - Save learned `Q`, `Q1`, and `Q2` tables to CSV for reuse  

---

## ğŸ§  Algorithmic Insight

### ğŸ”¹ Q-Learning Update Rule
\[
Q(s,a) \leftarrow Q(s,a) + \alpha [R(s,a) + \gamma \max_{a'} Q(s',a') - Q(s,a)]
\]

### ğŸ”¹ Double Q-Learning Update Rule
\[
Q_1(s,a) \leftarrow Q_1(s,a) + \alpha [R + \gamma Q_2(s', \arg\max_{a'} Q_1(s',a')) - Q_1(s,a)]
\]
\[
Q_2(s,a) \leftarrow Q_2(s,a) + \alpha [R + \gamma Q_1(s', \arg\max_{a'} Q_2(s',a')) - Q_2(s,a)]
\]

ğŸ§© The **Double Q-Learning** approach minimizes overestimation bias by splitting the value function estimation between two independent Q-tables.

---

## ğŸ“ˆ Visualizations

ğŸ¨ Generated automatically in the notebook:

- ğŸ—ºï¸ **Final Policy Map** â€“ optimal moves for each state  
- ğŸ“ˆ **Reward Convergence Plot** â€“ mean episodic reward progression  
- ğŸ”¥ **Exploration-Exploitation Trend** â€“ epsilon decay visualization  
- ğŸ§© **Trajectory Visualization** â€“ shows agent navigation paths  
- ğŸ’¾ **Q-Table Heatmap** â€“ visual structure of learned action-values  

---

## ğŸ’» Tech Stack

<p align="center">
  <img src="https://img.shields.io/badge/Language-Python-3776AB?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Libraries-NumPy%20%7C%20Matplotlib%20%7C%20Pandas-FFDD00?logo=plotly&logoColor=white" />
  <img src="https://img.shields.io/badge/Progress-tqdm-00C853" />
  <img src="https://img.shields.io/badge/Notebook-Jupyter%20%7C%20Colab-orange" />
</p>

---

## ğŸ§© How It Works

1ï¸âƒ£ Define the **Gridworld Environment** â€” including holes, obstacles, and rewards  
2ï¸âƒ£ Initialize **Q-tables** (or **Q1 & Q2**) with zeros  
3ï¸âƒ£ Apply **Îµ-greedy policy** for exploration vs. exploitation  
4ï¸âƒ£ Update Q-values iteratively based on agent experience  
5ï¸âƒ£ Visualize and **export learned policies and metrics**

---

## ğŸ§  Key Learnings

- **Double Q-Learning** stabilizes learning by reducing bias  
- **Reward shaping** impacts convergence speed and exploration  
- **Exploration decay** balances random exploration and exploitation  
- **Stateâ€“action heatmaps** reveal decision clustering  

---

## ğŸ§ª Future Enhancements

- ğŸ§  Integrate **Deep Q-Network (DQN)** for function approximation  
- ğŸ”„ Add **moving obstacles** or time-varying rewards  
- ğŸ•¹ï¸ Support **multi-agent learning and collaboration**  
- ğŸ¥ Generate **animated training visualization (GIF)**  
- ğŸ“Š Compare **SARSA vs. Q-Learning vs. Double Q-Learning**

---

## ğŸ‘¨â€ğŸ’» Author

**Sam Dennis**  
ğŸ“ MSc AI & ML â€” Christ University  
ğŸ’¼ Research Focus: Reinforcement Learning & IoT Security  
ğŸŒ [LinkedIn](https://www.linkedin.com/in/samdennis) â€¢ [GitHub](https://github.com/your-username)

---

> â€œTrue intelligence isnâ€™t about knowing the path â€” itâ€™s about learning to walk it.â€ ğŸŒ
